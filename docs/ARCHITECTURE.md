# ğŸ—ï¸ Platform Architecture

## Overview

The Cultural Heritage AI Platform is designed as a modular system where each component addresses a specific challenge in art and cultural heritage preservation. The platform leverages shared AI infrastructure while maintaining module independence.

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Cultural Heritage AI Platform                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    User Interface Layer                         â”‚   â”‚
â”‚  â”‚  (Jupyter Notebooks / Future: Web API / Gradio Interface)      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                      â”‚
â”‚                                    â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Application Modules                           â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚
â”‚  â”‚  â”‚   Art        â”‚  â”‚  Artistic   â”‚  â”‚  Heritage   â”‚           â”‚   â”‚
â”‚  â”‚  â”‚Authenticationâ”‚  â”‚   Image     â”‚  â”‚ Restoration â”‚           â”‚   â”‚
â”‚  â”‚  â”‚   Module     â”‚  â”‚ Generation  â”‚  â”‚   Module    â”‚           â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚
â”‚  â”‚         â”‚                 â”‚                  â”‚                    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                             â”‚   â”‚
â”‚  â”‚  â”‚   2D to 3D   â”‚  â”‚   Art Q&A   â”‚                             â”‚   â”‚
â”‚  â”‚  â”‚  Conversion  â”‚  â”‚   RAG System â”‚                             â”‚   â”‚
â”‚  â”‚  â”‚   Module     â”‚  â”‚    Module   â”‚                             â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                      â”‚
â”‚                                    â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Shared AI Infrastructure Layer                       â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚   â”‚
â”‚  â”‚  â”‚  Vision      â”‚  â”‚  Generative  â”‚  â”‚  Embedding   â”‚          â”‚   â”‚
â”‚  â”‚  â”‚ Transformers â”‚  â”‚    Models    â”‚  â”‚   Models     â”‚          â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ ViT       â”‚  â”‚  â€¢ SD XL     â”‚  â”‚  â€¢ Sentence   â”‚          â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Swin      â”‚  â”‚  â€¢ SD 1.0    â”‚  â”‚    Transform â”‚          â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ ResNet    â”‚  â”‚  â€¢ Hunyuan3D  â”‚  â”‚  â€¢ FAISS      â”‚          â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚   â”‚
â”‚  â”‚  â”‚ Fine-tuning  â”‚  â”‚  Image       â”‚  â”‚  Data         â”‚          â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ LoRA      â”‚  â”‚  Processing  â”‚  â”‚  Processing  â”‚          â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ PEFT      â”‚  â”‚  â€¢ Real-ESR  â”‚  â”‚  â€¢ Augment   â”‚          â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ QLoRA     â”‚  â”‚  â€¢ Waifu2x   â”‚  â”‚  â€¢ Preprocess â”‚          â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                      â”‚
â”‚                                    â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Data & Storage Layer                          â”‚   â”‚
â”‚  â”‚  â€¢ Hugging Face Hub  â€¢ Local Storage  â€¢ Cloud Storage (Azure)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Module Architecture Details

### 1. Art Authentication Module

**Architecture**: Multi-model ensemble approach

```
Input Image
    â”‚
    â”œâ”€â†’ CNN Branch (ResNet50)
    â”œâ”€â†’ Vision Transformer Branch
    â”œâ”€â†’ Swin Transformer Branch
    â””â”€â†’ Hybrid CNN+ViT Branch
         â”‚
         â””â”€â†’ Ensemble Voting
              â”‚
              â””â”€â†’ Output: AI/Human Classification + Confidence
```

**Key Components**:
- **Data Pipeline**: Augmentation, normalization, train/test split
- **Model Architectures**: CNN, ViT, Swin Transformer, ResNet50, Hybrid
- **Training**: Cross-entropy loss, Adam optimizer, learning rate scheduling
- **Evaluation**: Accuracy, Precision, Recall, F1-score

**Best Model**: Swin Transformer (91% test accuracy)

---

### 2. Artistic Image Generation Module

**Architecture**: RAG-enhanced Stable Diffusion

```
User Prompt + Artist Name
    â”‚
    â”œâ”€â†’ Sentence Transformer Embedding
    â”‚        â”‚
    â”‚        â””â”€â†’ FAISS Similarity Search
    â”‚                 â”‚
    â”‚                 â””â”€â†’ Retrieve Top-K Style Descriptions
    â”‚
    â””â”€â†’ Prompt Fusion Engine
         â”‚
         â”œâ”€â†’ User Prompt
         â”œâ”€â†’ Artist Name
         â””â”€â†’ Retrieved Style Descriptions
              â”‚
              â””â”€â†’ Fused Prompt
                   â”‚
                   â””â”€â†’ Stable Diffusion Pipeline
                        â”‚
                        â””â”€â†’ Generated Image (Artist Style)
```

**Key Components**:
- **Embedding Model**: SentenceTransformer (all-MiniLM-L6-v2)
- **Search Engine**: FAISS IndexFlatL2
- **Generation Model**: Stable Diffusion 1.0 (dreamlike-diffusion)
- **Fine-tuning**: Optional LoRA for custom styles

---

### 3. Heritage Restoration Module

**Architecture**: Multi-modal Conditional Diffusion

```
Damaged Image
    â”‚
    â”œâ”€â†’ Depth Map Generation (DPT Hybrid)
    â”œâ”€â†’ Image Captioning (Joy Transformer)
    â”œâ”€â†’ Damage Mask Detection (YOLO)
    â””â”€â†’ Feature Extraction
         â”‚
         â””â”€â†’ Multi-Modal Conditioning
              â”‚
              â”œâ”€â†’ Depth Map
              â”œâ”€â†’ Caption + Features
              â”œâ”€â†’ Damage Mask
              â””â”€â†’ Original Image
                   â”‚
                   â””â”€â†’ Stable Diffusion XL Inpainting
                        â”‚ (Fine-tuned with LoRA + PEFT)
                        â””â”€â†’ Restored Image (1024Ã—1024)
```

**Key Components**:
- **Base Model**: Stable Diffusion XL Inpainting
- **Fine-tuning**: LoRA (r=8, alpha=16) + PEFT
- **Conditioning**: Depth maps, captions, semantic masks
- **Preprocessing**: Real-ESRGAN (4Ã—), Waifu2x (denoising)
- **Training Data**: 40,000+ paired samples

**Training Pipeline**:
1. Data collection from multiple sources
2. De-duplication and similarity filtering
3. Super-resolution and enhancement
4. Depth map generation
5. Caption generation
6. Damage simulation
7. Model fine-tuning with LoRA

---

### 4. 2D to 3D Conversion Module

**Architecture**: Diffusion Transformer for 3D Generation

```
2D Statue Image
    â”‚
    â””â”€â†’ Hunyuan3D-DiT Pipeline
         â”‚
         â”œâ”€â†’ Image Encoding
         â”œâ”€â†’ Diffusion Process (Flow Matching)
         â””â”€â†’ 3D Mesh Generation
              â”‚
              â””â”€â†’ GLB Format Export
```

**Key Components**:
- **Model**: Hunyuan3D-2 (Tencent)
- **Architecture**: Diffusion Transformer (DiT)
- **Output Format**: GLB (GL Transmission Format Binary)
- **Post-processing**: Mesh optimization, texture mapping

---

### 5. Art Q&A RAG System

**Architecture**: Retrieval-Augmented Generation

```
User Question
    â”‚
    â””â”€â†’ Query Embedding (Sentence Transformer)
         â”‚
         â””â”€â†’ FAISS Semantic Search
              â”‚
              â””â”€â†’ Retrieve Relevant Context
                   â”‚
                   â”œâ”€â†’ Text Context
                   â””â”€â†’ Image Context (if applicable)
                        â”‚
                        â””â”€â†’ RAG Pipeline
                             â”‚
                             â”œâ”€â†’ LLM Generation (with context)
                             â””â”€â†’ Image Generation (if needed)
                                  â”‚
                                  â””â”€â†’ Combined Answer
```

**Key Components**:
- **Embedding Model**: SentenceTransformer
- **Vector Store**: FAISS
- **Generation**: LLM (via transformers) + Stable Diffusion
- **Knowledge Base**: Art descriptions, historical data, style information

---

## Data Flow

### Training Data Flow

```
Raw Data Sources
    â”‚
    â”œâ”€â†’ Wikimedia Commons
    â”œâ”€â†’ Smithsonian Museum API
    â”œâ”€â†’ Europeana API
    â””â”€â†’ MIT Museum Collections
         â”‚
         â””â”€â†’ Data Pipeline
              â”‚
              â”œâ”€â†’ De-duplication
              â”œâ”€â†’ Quality Filtering
              â”œâ”€â†’ Preprocessing
              â”‚    â”œâ”€â†’ Super-resolution
              â”‚    â”œâ”€â†’ Denoising
              â”‚    â””â”€â†’ Normalization
              â”‚
              â””â”€â†’ Feature Extraction
                   â”‚
                   â”œâ”€â†’ Depth Maps
                   â”œâ”€â†’ Captions
                   â”œâ”€â†’ Embeddings
                   â””â”€â†’ Metadata
                        â”‚
                        â””â”€â†’ Training Dataset
```

### Inference Data Flow

```
User Input
    â”‚
    â””â”€â†’ Module Selection
         â”‚
         â”œâ”€â†’ Art Authentication
         â”‚    â””â”€â†’ Image â†’ Model â†’ Classification
         â”‚
         â”œâ”€â†’ Image Generation
         â”‚    â””â”€â†’ Prompt â†’ RAG â†’ Generation â†’ Image
         â”‚
         â”œâ”€â†’ Heritage Restoration
         â”‚    â””â”€â†’ Image â†’ Conditioning â†’ Restoration â†’ Image
         â”‚
         â”œâ”€â†’ 2D to 3D
         â”‚    â””â”€â†’ Image â†’ 3D Pipeline â†’ Mesh
         â”‚
         â””â”€â†’ Q&A System
              â””â”€â†’ Question â†’ RAG â†’ Answer
```

## Model Sharing & Reusability

The platform is designed with shared components:

1. **Embedding Models**: Used across RAG, Image Generation, and Q&A
2. **Vision Transformers**: Shared between Authentication and Restoration
3. **Stable Diffusion**: Base models shared, fine-tuned per module
4. **Preprocessing**: Common image processing utilities

## Scalability Considerations

- **Model Loading**: Lazy loading of large models
- **Caching**: FAISS indices and embeddings cached
- **Batch Processing**: Support for batch inference
- **GPU Utilization**: Efficient GPU memory management
- **Distributed Training**: Support for multi-GPU training

## Future Architecture Enhancements

1. **API Layer**: RESTful API for all modules
2. **Web Interface**: Gradio/Streamlit dashboard
3. **Model Serving**: TorchServe or TensorFlow Serving
4. **Database**: Vector database (Pinecone, Weaviate) for RAG
5. **Microservices**: Containerized modules (Docker)
6. **Orchestration**: Kubernetes for scaling

## Performance Metrics

| Module | Latency | Throughput | GPU Memory |
|--------|---------|------------|------------|
| Art Authentication | ~50ms | 20 img/s | 4GB |
| Image Generation | ~3s | 0.3 img/s | 8GB |
| Heritage Restoration | ~5s | 0.2 img/s | 12GB |
| 2D to 3D | ~30s | 0.03 mesh/s | 16GB |
| RAG Q&A | ~1s | 1 query/s | 2GB |

*Metrics measured on NVIDIA RTX 3090*

---

## Security & Privacy

- **Data Privacy**: No user data stored permanently
- **Model Security**: Signed model checkpoints
- **API Security**: Rate limiting, authentication (future)
- **Content Filtering**: NSFW filtering for generated content

---

## Deployment Architecture (Future)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Load      â”‚
â”‚  Balancer   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”
   â”‚  API  â”‚
   â”‚Gatewayâ”‚
   â””â”€â”€â”€â”¬â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                               â”‚
â”Œâ”€â”€â”´â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”´â”€â”€â”
â”‚Auth â”‚  â”‚Image â”‚  â”‚Restoreâ”‚  â”‚2D-3D  â”‚
â”‚Moduleâ”‚  â”‚Gen   â”‚  â”‚Moduleâ”‚  â”‚Module â”‚
â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

For implementation details of each module, see the respective module guides in `docs/modules/`.

