{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgO1T2zjh_iN",
        "outputId": "36c80bba-fdef-4821-a0eb-160e39c3a86b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: azure-storage-blob in /usr/local/lib/python3.11/dist-packages (12.25.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: azure-core>=1.30.0 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob) (1.33.0)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob) (43.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob) (4.13.0)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob) (0.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure-storage-blob) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.22)\n",
            "üîç Searching Smithsonian for 'bust'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching Smithsonian metadata: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9289/9289 [01:02<00:00, 147.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Found 9289 items\n",
            "Proceed with processing and upload? (y/n): y\n",
            "‚è≥ Processing 9289 items...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9289/9289 [00:00<00:00, 186640.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üèÅ Completed! Successfully processed 0/9289 items\n",
            "üì¶ Files uploaded to Azure Blob Storage container: smithsonianbust\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install azure-storage-blob requests tqdm\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Configuration\n",
        "SEARCH_QUERY = \"Your query\"\n",
        "SMITHSONIAN_API_KEY = \"YOUR_SMITHSONIAN_API_KEY\"\n",
        "API_URL = \"https://api.si.edu/openaccess/api/v1.0/search\"\n",
        "RESULTS_PER_REQUEST = 1000\n",
        "MAX_WORKERS = 4\n",
        "\n",
        "# Azure Configuration\n",
        "connection_string = \"your azure connection string\"\n",
        "container_name = \"your azure blob container name\"\n",
        "\n",
        "# Initialize Azure client\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "\n",
        "def sanitize_filename(filename, is_json=False):\n",
        "    \"\"\"Create safe blob names with proper extensions\"\"\"\n",
        "    clean = re.sub(r'[^a-zA-Z0-9_-]', '', str(filename).replace(' ', '_'))[:150]\n",
        "    if is_json:\n",
        "        return f\"{clean}.json\"\n",
        "    return f\"{clean}.jpg\"\n",
        "\n",
        "def upload_to_blob(data, filename, is_json=False):\n",
        "    \"\"\"Enhanced blob upload with error handling\"\"\"\n",
        "    blob_name = sanitize_filename(filename, is_json)\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "    try:\n",
        "        blob_client.upload_blob(data, overwrite=True)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Upload failed for {blob_name}: {str(e)[:200]}\")\n",
        "        return False\n",
        "\n",
        "def fetch_smithsonian_metadata():\n",
        "    \"\"\"Fetch all matching items from Smithsonian API with better error handling\"\"\"\n",
        "    params = {\n",
        "        'api_key': SMITHSONIAN_API_KEY,\n",
        "        'q': SEARCH_QUERY,\n",
        "        'rows': RESULTS_PER_REQUEST,\n",
        "        'start': 0,\n",
        "        'fq': 'online_media_type:Images AND media_usage:CC0'\n",
        "    }\n",
        "\n",
        "    all_items = []\n",
        "    total_results = None\n",
        "\n",
        "    with tqdm(desc=\"Fetching Smithsonian metadata\") as pbar:\n",
        "        while True:\n",
        "            try:\n",
        "                response = requests.get(API_URL, params=params, timeout=60)\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "\n",
        "                if not isinstance(data.get('response', {}).get('rows', []), list):\n",
        "                    print(\"Unexpected API response format\")\n",
        "                    break\n",
        "\n",
        "                if total_results is None:\n",
        "                    total_results = min(data['response'].get('rowCount', 0), 50000)  # Safety cap\n",
        "                    pbar.total = total_results\n",
        "\n",
        "                items = data['response']['rows']\n",
        "                all_items.extend(items)\n",
        "                pbar.update(len(items))\n",
        "\n",
        "                if len(all_items) >= total_results or len(items) < RESULTS_PER_REQUEST:\n",
        "                    break\n",
        "\n",
        "                params['start'] += RESULTS_PER_REQUEST\n",
        "                time.sleep(0.5)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching data: {str(e)[:200]}\")\n",
        "                break\n",
        "\n",
        "    return all_items\n",
        "\n",
        "def safe_get(data, *keys, default=None):\n",
        "    \"\"\"Safely navigate nested dictionaries\"\"\"\n",
        "    for key in keys:\n",
        "        try:\n",
        "            data = data[key]\n",
        "        except (TypeError, KeyError, AttributeError):\n",
        "            return default\n",
        "    return data\n",
        "\n",
        "def process_smithsonian_item(item):\n",
        "    \"\"\"Robust item processing with comprehensive error handling\"\"\"\n",
        "    try:\n",
        "        if not isinstance(item, dict):\n",
        "            return False\n",
        "\n",
        "        item_id = safe_get(item, 'id', default='')\n",
        "        title = safe_get(item, 'title', default='Untitled')\n",
        "\n",
        "        # Safely extract creator information\n",
        "        creator = ''\n",
        "        name_data = safe_get(item, 'name', default=[])\n",
        "        if isinstance(name_data, list) and len(name_data) > 0:\n",
        "            creator = safe_get(name_data[0], 'display', default='')\n",
        "        elif isinstance(name_data, dict):\n",
        "            creator = safe_get(name_data, 'display', default='')\n",
        "\n",
        "        # Build metadata\n",
        "        metadata = {\n",
        "            \"title\": title,\n",
        "            \"creator\": creator,\n",
        "            \"date\": safe_get(item, 'date', default=''),\n",
        "            \"culture\": safe_get(item, 'culture', default=''),\n",
        "            \"medium\": safe_get(item, 'medium', default=''),\n",
        "            \"collection\": safe_get(item, 'data_source', default=''),\n",
        "            \"object_type\": safe_get(item, 'object_type', default=''),\n",
        "            \"credit_line\": safe_get(item, 'credit_line', default=''),\n",
        "            \"url\": f\"https://www.si.edu/object/{item_id}\"\n",
        "        }\n",
        "\n",
        "        # Find best image URL\n",
        "        image_url = None\n",
        "        content_data = safe_get(item, 'content', default=[])\n",
        "        if isinstance(content_data, list):\n",
        "            for content in content_data:\n",
        "                if (isinstance(content, dict) and\n",
        "                    content.get('type') == 'Images' and\n",
        "                    content.get('usage') == 'CC0'):\n",
        "                    image_url = content.get('url', '')\n",
        "                    if image_url and 'ids.si.edu' in image_url:\n",
        "                        break\n",
        "\n",
        "        if not image_url:\n",
        "            return False\n",
        "\n",
        "        # Upload metadata\n",
        "        metadata_bytes = json.dumps(metadata, indent=2).encode('utf-8')\n",
        "        if not upload_to_blob(metadata_bytes, f\"{item_id}_metadata\", is_json=True):\n",
        "            return False\n",
        "\n",
        "        # Download and upload image\n",
        "        try:\n",
        "            response = requests.get(image_url, stream=True, timeout=60)\n",
        "            response.raise_for_status()\n",
        "            if not upload_to_blob(response.content, f\"{item_id}_{title}\"):\n",
        "                return False\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Image download failed for {item_id}: {str(e)[:200]}\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {item_id}: {str(e)[:200]}\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    print(f\"üîç Searching Smithsonian for '{SEARCH_QUERY}'...\")\n",
        "    items = fetch_smithsonian_metadata()\n",
        "\n",
        "    if not items:\n",
        "        print(\"‚ùå No items found\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n‚úÖ Found {len(items)} items\")\n",
        "    confirm = input(\"Proceed with processing and upload? (y/n): \")\n",
        "\n",
        "    if confirm.lower() != 'y':\n",
        "        print(\"üö´ Operation cancelled\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚è≥ Processing {len(items)} items...\")\n",
        "    success_count = 0\n",
        "\n",
        "    # Process items in parallel with progress tracking\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        futures = [executor.submit(process_smithsonian_item, item) for item in items]\n",
        "        for future in tqdm(as_completed(futures), total=len(items), desc=\"Processing\"):\n",
        "            success_count += future.result()\n",
        "\n",
        "    print(f\"\\nüèÅ Completed! Successfully processed {success_count}/{len(items)} items\")\n",
        "    print(f\"üì¶ Files uploaded to Azure Blob Storage container: {container_name}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
